# ğŸ­ The NextWord Predictor Using LSTM GRU

Predict the **next word** in Shakespeareâ€™s *Hamlet* using two recurrent architecturesâ€”**LSTM** and **GRU**â€”and compare how they behave. Built for clarity, speed, and fun experimentation.

> *â€œGoodnight, sweet Princeâ€¦â€* â€” Yep, the corpus is **Hamlet (public domain)**, and thatâ€™s our playground. :contentReference[oaicite:0]{index=0} :contentReference[oaicite:1]{index=1}

---

## ğŸ”¥ Highlights

- **Two model families:** Long Short-Term Memory (**LSTM**) and Gated Recurrent Unit (**GRU**)
- **Side-by-side training** in notebooks + **one-click inference** in a Streamlit app  
- **Plug-and-play assets:** saved model(s) + tokenizer for instant use
- **Reproducible stack:** pinned core libs in `requirements.txt` (TensorFlow, Streamlit, NLTK, etc.) :contentReference[oaicite:2]{index=2}
- **Shakespearean flavor:** trained on *Hamlet* (full text included) :contentReference[oaicite:3]{index=3}

---

## ğŸ—‚ï¸ Repository Structure
